{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YZV302E - DEEP LEARNING TERM PROJECT\n",
    "## Many-to-one Voice Conversion\n",
    "## Fall, 23-24, Istanbul Technical University\n",
    "\n",
    "\n",
    "### Authors: Muhammet Serdar NAZLI, Ã–mer Faruk AYDIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS OUR FAILED APPROACH. YOU WILL NOT BE ABLE TO GET GOOD RESULTS, THE MODEL WILL NOT CONVERGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries and device settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.18 (default, Sep 11 2023, 13:39:12) [MSC v.1916 64 bit (AMD64)]\n",
      "Using CPU/GPU:  cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(sys.version)\n",
    "import glob\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.functional import F\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pretrained.rmvpe.RMVPE.rmvpe import RMVPE\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Using CPU/GPU: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuBERT Large model from torch pipelines.\n",
    "\n",
    "For more details you can check the paper about the HuBERT: https://arxiv.org/abs/2106.07447\n",
    "\n",
    "For more details about pipeline to get pretrained HuBERT: 'https://pytorch.org/audio/0.10.0/pipelines.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more details about what this pipelines lib is doing, see: torchaudio>pipelines>_wav2vec>impl.py: HUBERT_BASE. \n",
    "hubert_bundle = torchaudio.pipelines.HUBERT_BASE              \n",
    "\n",
    "# Build the model and load pre-trained weights\n",
    "# Around 300MB. May take a while to download if you have a slow connection.\n",
    "# Once you get it, it will be cached. You won't have to download it again.\n",
    "hubert_model = hubert_bundle.get_model().to(device)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMPVE, Real-time Monophonic Vocal Pitch Extractor through local files.\n",
    "\n",
    "File is around 180MB. \n",
    "\n",
    "For more details you can check the paper:\n",
    "'https://arxiv.org/abs/2306.15412' <br>\n",
    "implementation: 'https://github.com/Dream-High/RMVPE?tab=readme-ov-file' <br>\n",
    "huggingface .pt file link: 'https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/rmvpe.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting it from the local files. RMVPE class definition is in the pretrained/rmvpe/RMVPE/rmvpe.py file.\n",
    "# Pretrained model is in the pretrained/rmvpe/RMVPE/rmvpe.pt file which is an OrderedDict.\n",
    "rmpve_model = RMVPE(model_path='pretrained/rmvpe/RMVPE/rmvpe.pt', is_half=False, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmpve_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hifi-GAN through Nvidia pipelines\n",
    "\n",
    "For more details you can check the paper: 'https://arxiv.org/abs/2010.05646' \n",
    "\n",
    "pretrained model details: 'https://huggingface.co/nvidia/tts_hifigan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hifigan, vocoder_train_setup, denoiser = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_hifigan')\n",
    "hifigan_model = hifigan.to(device)\n",
    "denoiser_model = denoiser.to(device)\n",
    "\n",
    "print(\"\\nHifiGAN Model:\\n\",hifigan_model)\n",
    "print(\"\\nDenoiser:\\n\",denoiser_model)\n",
    "print(vocoder_train_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class / Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import librosa\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VoiceConversionDataset(Dataset):\n",
    "    def __init__(self, file_paths, hubert_model, rmvpe_model, device, n_mels=80):\n",
    "        \"\"\"\n",
    "        file_paths: List of paths to audio files.\n",
    "        hubert_model: Pre-trained HuBERT model.\n",
    "        rmvpe_model: Pre-trained RMVPE model.\n",
    "        device: Torch device to run the models on.\n",
    "        n_mels: Number of mel frequency bands (default 80).\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.hubert_model = hubert_model\n",
    "        self.rmvpe_model = rmvpe_model\n",
    "        self.device = device\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and preprocess audio\n",
    "        audio, sampling_rate = sf.read(self.file_paths[idx])\n",
    "        orig_audio = audio.copy()   \n",
    "        if len(audio.shape) > 1:\n",
    "            audio = librosa.to_mono(audio.transpose(1, 0))\n",
    "        if sampling_rate != 16000:\n",
    "            audio = librosa.resample(audio, orig_sr=sampling_rate, target_sr=16000)\n",
    "\n",
    "        # Convert to tensor and add batch dimension\n",
    "        audio_tensor = torch.from_numpy(audio).float().unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            self.hubert_model.eval()\n",
    "            hubert_features,_ = self.hubert_model(audio_tensor)  # Remove batch dimension\n",
    "            hubert_features = hubert_features.squeeze(0)\n",
    "            rmvpe_features = self.rmvpe_model.infer_from_audio(audio, thred=0.03)\n",
    "            rmvpe_features = torch.tensor(rmvpe_features).float()  # Convert to tensor if not already\n",
    "\n",
    "\n",
    "        # Generate target mel spectrogram\n",
    "        # audio, sr = librosa.load(self.file_paths[idx], sr=22050)  # Sample rate 22050 Hz\n",
    "\n",
    "        # Extract mel-spectrogram\n",
    "        \"\"\"mel_spectrogram = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=sr,\n",
    "            n_fft=1024,            # FFT window size\n",
    "            hop_length=256,        # Window stride\n",
    "            n_mels=80,             # Number of Mel bands\n",
    "            fmin=0,                # Minimum frequency\n",
    "            fmax=8000,             # Maximum frequency\n",
    "            window='hann'          # Window type\n",
    "        )\"\"\"\n",
    "        \n",
    "        #mel_spec = librosa.feature.melspectrogram(y=audio, sr=sampling_rate, n_mels=self.n_mels)\n",
    "        #mel_spec = torch.from_numpy(mel_spec).float()\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=sr,\n",
    "            n_fft=1024,            # FFT window size\n",
    "            hop_length=256,        # Window stride\n",
    "            n_mels=80,             # Number of Mel bands\n",
    "            fmin=0,                # Minimum frequency\n",
    "            fmax=8000,             # Maximum frequency\n",
    "            window='hann'          # Window type\n",
    "        )\n",
    "        mel_spec = torch.from_numpy(mel_spectrogram).float()\n",
    "\n",
    "        assert mel_spec.shape[0] == self.n_mels, f\"Mel spectrogram shape mismatch {mel_spec.shape} != {self.n_mels}\"\n",
    "        \n",
    "        return hubert_features, rmvpe_features, mel_spec\n",
    "\n",
    "    \n",
    "def pad_sequences_1d(sequences):\n",
    "    \"\"\"Pad 1D sequences to the maximum length sequence in the batch.\"\"\"\n",
    "    max_len = max([s.size(0) for s in sequences])  # Assuming each sequence in sequences is already a tensor\n",
    "    padded_sequences = torch.zeros((len(sequences), max_len))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        end = seq.size(0)\n",
    "        padded_sequences[i, :end] = seq  # Directly using the tensor\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "def pad_sequences_2d(sequences, pad_dim=1):\n",
    "    \"\"\"Pad 2D sequences to the maximum length sequence in the batch.\"\"\"\n",
    "    max_len = max([s.size(pad_dim) for s in sequences])\n",
    "    other_dim = sequences[0].size(1 - pad_dim)\n",
    "\n",
    "    padded_sequences = torch.zeros((len(sequences), max_len, other_dim) if pad_dim == 0 else (len(sequences), other_dim, max_len))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(pad_dim)\n",
    "        if pad_dim == 0:\n",
    "            padded_sequences[i, :length, :] = seq[:length, :]\n",
    "        else:\n",
    "            padded_sequences[i, :, :length] = seq[:, :length]\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    hubert_features, rmvpe_features, target_mels = zip(*batch)\n",
    "\n",
    "    padded_hubert = pad_sequences_2d(hubert_features, pad_dim=0)  # Pad along time dimension (dim 0 for HuBERT)\n",
    "    padded_rmvpe = pad_sequences_1d(rmvpe_features)\n",
    "    padded_target_mels = pad_sequences_2d(target_mels, pad_dim=1)  # Pad along time dimension (dim 1 for Mel)\n",
    "\n",
    "    return padded_hubert, padded_rmvpe, padded_target_mels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path\n",
    "directory_path = \"data-/\"\n",
    "\n",
    "# Get all the files in the directory that end with .wav\n",
    "file_paths = glob.glob(directory_path + \"/*.wav\")\n",
    "\n",
    "\n",
    "\n",
    "voice_conversion_dataset = VoiceConversionDataset(file_paths[:], hubert_model, rmpve_model, device)\n",
    "\n",
    "voice_loader = DataLoader(voice_conversion_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for i,batch in enumerate(voice_loader):\n",
    "    padded_hubert, padded_rmvpe, padded_target_mels = batch\n",
    "    print(f\"(batch{i}) Padded HuBERT Features:\", padded_hubert.shape)\n",
    "    print(f\"(batch{i}) Padded RMVPE Features:\", padded_rmvpe.shape)\n",
    "    print(f\"(batch{i}) Padded Target Mel Spectrograms:\", padded_target_mels.shape)\n",
    "    if i == 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, hubert_feature_size=1024, rmvpe_feature_size=1, num_mels=80, hidden_size=256):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        # Encoder LSTM layers\n",
    "        self.encoder_lstm1 = nn.LSTM(\n",
    "            input_size=hubert_feature_size + rmvpe_feature_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.encoder_batchnorm1 = nn.BatchNorm1d(hidden_size * 2)  # *2 for bidirectional output\n",
    "        self.encoder_lstm2 = nn.LSTM(\n",
    "            input_size=hidden_size * 2, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.encoder_batchnorm2 = nn.BatchNorm1d(hidden_size * 2)\n",
    "\n",
    "        # Additional layer for processing encoded features\n",
    "        self.encoded_linear = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "        # Decoder LSTM layers\n",
    "        self.decoder_lstm1 = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder_batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.decoder_lstm2 = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder_batchnorm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.decoder_linear = nn.Linear(hidden_size, num_mels)\n",
    "\n",
    "    def forward(self, hubert_features, rmvpe_features, target_time_steps):\n",
    "        batch_size, time_steps, _ = hubert_features.size()\n",
    "\n",
    "        # Resample RMVPE features to match the time steps of HuBERT features\n",
    "        rmvpe_features = F.interpolate(rmvpe_features.unsqueeze(1), size=time_steps, mode='linear', align_corners=False)\n",
    "        rmvpe_features = rmvpe_features.squeeze(1)\n",
    "\n",
    "        # Concatenate HuBERT and RMVPE features\n",
    "        combined_features = torch.cat((hubert_features, rmvpe_features.unsqueeze(-1)), dim=-1)\n",
    "\n",
    "        # Encoding\n",
    "        encoded_features, _ = self.encoder_lstm1(combined_features)\n",
    "        encoded_features = encoded_features.contiguous().transpose(1, 2)\n",
    "        encoded_features = self.encoder_batchnorm1(encoded_features)\n",
    "        encoded_features = encoded_features.transpose(1, 2)\n",
    "        encoded_features, _ = self.encoder_lstm2(encoded_features)\n",
    "        encoded_features = encoded_features.contiguous().transpose(1, 2)\n",
    "        encoded_features = self.encoder_batchnorm2(encoded_features)\n",
    "        encoded_features = encoded_features.transpose(1, 2)\n",
    "\n",
    "        # Process encoded features\n",
    "        encoded_features = self.encoded_linear(encoded_features)\n",
    "\n",
    "        # Decoding\n",
    "        decoded_features, _ = self.decoder_lstm1(encoded_features)\n",
    "        decoded_features = decoded_features.contiguous().transpose(1, 2)\n",
    "        decoded_features = self.decoder_batchnorm1(decoded_features)\n",
    "        decoded_features = decoded_features.transpose(1, 2)\n",
    "        decoded_features, _ = self.decoder_lstm2(decoded_features)\n",
    "        decoded_features = decoded_features.contiguous().transpose(1, 2)\n",
    "        decoded_features = self.decoder_batchnorm2(decoded_features)\n",
    "        decoded_features = decoded_features.transpose(1, 2)\n",
    "        decoded_features = self.decoder_linear(decoded_features)\n",
    "\n",
    "        # Transpose to bring mel bands dimension before time steps\n",
    "        decoded_features = decoded_features.transpose(1, 2)\n",
    "\n",
    "        # Ensure output time dimension matches target\n",
    "        mel_output = F.interpolate(decoded_features, size=(target_time_steps), mode='linear', align_corners=False)\n",
    "\n",
    "        return mel_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Eval Loops/Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            # Unpack the data\n",
    "            hubert_features, rmvpe_features, target_mels = data\n",
    "            hubert_features, rmvpe_features, target_mels = hubert_features.to(device), rmvpe_features.to(device), target_mels.to(device)\n",
    "\n",
    "            # Get target time steps\n",
    "            target_time_steps = target_mels.size(2)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(hubert_features, rmvpe_features, target_time_steps)\n",
    "\n",
    "            # Calculate loss and perform backpropagation\n",
    "            loss = criterion(outputs, target_mels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:\n",
    "                print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for batch in dataloader:\n",
    "            inputs, targets = batch\n",
    "            \n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "autoencoder = AutoEncoder()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss = train(autoencoder, voice_loader, optimizer, criterion, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in voice_loader:\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        hubert_features, rmvpe_features, target_mels = item\n",
    "        hubert_features, rmvpe_features, target_mels = hubert_features.to(device), rmvpe_features.to(device), target_mels.to(device)\n",
    "\n",
    "        # Get target time steps\n",
    "        target_time_steps = target_mels.size(2)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = autoencoder(hubert_features, rmvpe_features, target_time_steps)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.cpu()[0].shape\n",
    "mel_spectrogram_db = outputs.cpu()[0].numpy()\n",
    "target_mels.cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize mel-spectrogram to be between -10 and 10\n",
    "# mel_spectrogram_db = target_mels[0].cpu().numpy()\n",
    "min_level = np.min(mel_spectrogram_db)\n",
    "max_level = np.max(mel_spectrogram_db)\n",
    "mel_spectrogram_normalized =  (mel_spectrogram_db - min_level) / (max_level - min_level) * 10 - 10\n",
    "\n",
    "\n",
    "# Ensure the mel-spectrogram is in the correct format for HiFi-GAN\n",
    "mel_spectrogram_torch = torch.tensor(mel_spectrogram_normalized).unsqueeze(0).to(torch.float32).to(device)  # Add batch dimension and convert to float32\n",
    "\n",
    "\n",
    "# Generate audio from mel-spectrogram\n",
    "with torch.no_grad():\n",
    "    audio_output = hifigan_model(mel_spectrogram_torch).float()\n",
    "    audio_output = denoiser_model(audio_output.squeeze(1), 0.01)\n",
    "    audio_output = audio_output.squeeze(1) * vocoder_train_setup['max_wav_value']\n",
    "\n",
    "print(mel_spectrogram_torch.shape)\n",
    "# Convert to numpy and normalize\n",
    "audio_numpy = audio_output.cpu().numpy()\n",
    "audio_numpy = np.int16(audio_numpy / np.max(np.abs(audio_numpy)) * 22050)\n",
    "Audio(audio_numpy, rate=22050)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
